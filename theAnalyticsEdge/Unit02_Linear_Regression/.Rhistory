?cv.glm
?loocv
loocv=function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
## Now we try it out
loocv(glm.fit)
cv.error=rep(0,5)
degree=1:5
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d]=loocv(glm.fit)
}
plot(degree,cv.error,type="b")
## 10-fold CV
cv.error10=rep(0,5)
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error10[d]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
lines(degree,cv.error10,type="b",col="red")
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)
## What is the standard error of alpha?
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
Xy
names(Xy)
?Xy
fit1=lm(y~.,data=Xy)
fit1
summary(fit1)
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
matplot(Xy, type = "l", legend = T)
Xy
names(Xy)
?Xy
fit1=lm(y~.,data=Xy)
fit1
summary(fit1)
matplot(Xy, type = "l", legend = T)
?matplot
matplot(Xy, type = "l", legend = T)
alpha(X1,y)
alpha(Xy$X1,Xy$y)
## What is the standard error of alpha?
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Xy,1:100)
?with
alpha.fn=function(data, index){
with(data[index,],alpha(X1,Y))
}
with(data[index,],alpha(x1,y))
alpha.fn=function(data, index){
with(data[index,],alpha(x1,y))
}
alpha.fn(Xy,1:100)
alpha.fn=function(data, index){
with(data[index,],alpha(Xy$x1,y))
}
alpha.fn(Xy,1:100)
alpha.fn=function(data, index){
with(data[index,],alpha(Xy$X1,Xy$y))
}
alpha.fn(Xy,1:100)
set.seed(1)
alpha.fn (Xy,sample(1:100,100,replace=TRUE))
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
plot(boot.out)
?rep
library(SDSFoundations)
survey <- StudentSurvey
library(SDSFoundations)
survey <- StudentSurvey
View(survey)
chr(survey)
str(survey)
fivenum(survey$name_letters)
survey$name_letters.mean()
mean(survey$name_letters)
s
mean(survey$name_letters)
chr()
mean(survey$name_letters)
summary(survey$name_letters)
sd(survey$name_letters)
hist(survey$name_letters)
fivenum(survey$name_letters)
xbar5 <-rep(NA, 1000)
for (i in 1:1000)
{x <-sample(survey$name_letters, size =5)
xbar5[i] <-  mean(x)}
hist(xbar5,xlim=c(2,10))
mean(xbar5)
sd(xbar5)
sd(survey$name_letters)/sqrt(5)
mean(survey$name_letters)
sd(survey$name_letters)
xbar5 <-rep(NA, 1000)
for (i in 1:1000)
{x <-sample(survey$name_letters, size =5)
xbar5[i] <-  mean(x)}
# Graph the histogram of 1,000 sample means.
hist(xbar5,xlim=c(2,10))
# Calculate the mean and sd of the sampling distribution.
mean(xbar5)
sd(xbar5)
# Compare to the std dev predicted by the CTL.
sd(survey$name_letters)/sqrt(5)
#Repeat for samples of size n=15
xbar15 <-rep(NA, 1000)
for (i in 1:1000)
{x <-sample(survey$name_letters, size =15)
xbar15[i] <- mean(x)}
hist(xbar15,xlim=c(2,10))
mean(xbar15)
sd(xbar15)
sd(survey$name_letters)/sqrt(15)
#Repeat for samples of size n=25
xbar25 <-rep(NA, 1000)
for (i in 1:1000)
{x <-sample(survey$name_letters, size =25)
xbar25[i] <- mean(x)}
hist(xbar25,xlim=c(2,10))
mean(xbar25)
sd(xbar25)
sd(survey$name_letters)/sqrt(25)
hist(survey$happy)
fivenum(survey$happy)
mean(survey$happy)
sd(survey$happy)
xbar5 <-rep(NA, 1000)
for (i in 1:1000)
{x <-sample(survey$happy, size =5)
xbar5[i] <-  mean(x)}
# Graph the histogram of 1,000 sample means.
hist(xbar5,xlim=c(50,100))
# Calculate the mean and sd of the sampling distribution.
mean(xbar5)
sd(xbar5)
# Compare to the std dev predicted by the CTL.
sd(survey$happy)/sqrt(5)
#Repeat for samples of size n=15
xbar15 <-rep(NA, 1000)
for (i in 1:1000)
{x <-sample(survey$happy, size =15)
xbar15[i] <- mean(x)}
hist(xbar15,xlim=c(50,100))
mean(xbar15)
sd(xbar15)
sd(survey$happy)/sqrt(15)
#Repeat for samples of size n=25
xbar25 <-rep(NA, 1000)
for (i in 1:1000)
{x <-sample(survey$happy, size =25)
xbar25[i] <- mean(x)}
hist(xbar25,xlim=c(50,100))
mean(xbar25)
sd(xbar25)
sd(survey$happy)/sqrt(25)
library(SDSFoundations)
survey <- StudentSurvey
str(survey)
hist(survey$austin)
mean(survey$austin)
sd(survey$austin)
sd(survey$austin) / sqrt(10)
xbar10 <-rep(NA, 1000)
for (i in 1:1000)
{x <-sample(survey$austin, size =10)
xbar10[i] <- mean(x)}
hist(xbar10,xlim=c(0,15))
qt(c(.025, .975), df=24)
qt(c(.025, .975), df=6)
qt(0.05, df=6)
library(SDSFoundations)
bull <- BullRiders
str(bull)
USA <-bull[bull$Country=="USA",]
# Summarize the bull rider weights
mean(USA$Weight)
sd(USA$Weight)
# Visualize the weight distribution
hist(USA$Weight, main='Histogram of US Bull Rider Weights',xlab='Weight (lbs)')
# Run the single sample t-test
t.test(USA$Weight, mu=190)
View(USA)
View(USA)
hist(bull$YearBorn, main = 'Histogram of Bull Rider Weights, xlab= 'Weight (lbs)')
mean(USA$Weight)
sd(USA$Weight)
t.test(USA$Weight, mu=190)
View(bull)
rider2014_5 = bull[bull$Events14 >= 5, ]
mean(rider2014_5$RidePer14)
sd(rider2014_5$RidePer14)
t.test(rider2014_5$RidePer14, mu=0.5)
hist(rider2014_5$RidePer14, main='Histogram of US Bull Rider Weights',xlab='Weight (lbs)')
bull$earnings_per = bull$Earnings12/bull$Events12
View(bull)
hist(rider2014_5$RidePer14,xlab='Weight (lbs)')
hist(bull$earnings_per)
bull$newvariable <- log(bull$originalvariable)
bull$newvariable <- log(bull$earnings_per)
hist(bull$newvariable)
mean(bull$newvariable)
?mean
mean(bull$newvariable, na.rm = FALSE)
mean(bull$newvariable, na.rm = TRUE)
t.test(bull$newvariable, mu=8.846387)
exp(8.572169 )
exp(9.120605)
potatochips = c(29.4	29	28.4	28.8	28.9	29.3	28.5	28.2)
potatochips = c(29.4, 29, 28.4, 28.8, 28.9, 29.3, 28.5, 28.2)
mean(potatochips)
sd(potatochips)
t.test(potatochips, mu=28.5)
qt(c(.025, .975), df=7)
qt(.95, df=24)
qt(c(0.05, 0.95), df=11)
qt(c(0.025, 0.975), 9)
qt(0.05, 9)
qt(0.95, 9)
sd(c(-1, 1, -2, -2))
sd(c(-1, 1, -2, -2))/2
qt(c(0.025, 0.975), 3)
qt(0.95, 3)
qt(0.05, 3)
qt(0.05, 4)
qt(0.05, 3)
qt(c(0.025, 0.975), 4)
qt(c(0.025, 0.975), 3)
t.test(c(-1, 1, -2, -2), 0)
t.test(c(-1, 1, -2, -2), mu = 0)
library(SDSFoundations)
post <- PostSurvey
library(SDSFoundations)
post <- PostSurvey
View(post)
underclass_happy <- post$happy[post$classification=='Freshman'|post$classification=='Sophomore']
upperclass_happy <- post$happy[post$classification=='Junior'|post$classification=='Senior']
# Check the normality assumption
hist(underclass_happy, xlab='Underclassman Happiness', main='Percent of Time Happy')
hist(upperclass_happy, xlab='Upperclassman Happiness', main='Percent of Time Happy')
t.test(underclass_happy, upperclass_happy)
mean(underclass_happy)
post$diff_happy <- post$happy - post$post_happy
# Check the normality assumption
hist(post$diff_happy, xlab= 'Difference in Happiness over the Semester', main = 'Happy-Post Happy')
# Run dependent t-test
t.test(post$happy, post$post_happy, paired=T)
post$diff_hw = post$hw_hours_college - post$hw_hours_HS
mean(post$diff_hw)
t.test(post$hw_hours_college, post$hw_hours_HS, alternative = 'greater')
?t.test
t.test(post$hw_hours_college, post$hw_hours_HS, alternative = 'greater', paired = TRUE)
t.test(post$diff_hw, alternative = 'greater')
greek = post[post$greek == yes, ]
greek = post[post$greek == "yes", ]
nongreek = post[post$greek == "no", ]
mean(greek$sleep_Sat) - mean(nongreek$sleep_Sat)
t.test(greek$sleep_Sat, nongreek$sleep_Sat, alternative = 'less')
hist(greek$sleep_Sat)
hist(nongreek$sleep_Sat)
hist(post$diff_hw)
hist(greek$sleep_Sat)
hist(nongreek$sleep_Sat)
nursing = post[post$major == "Nursing", ]
biology = post[post$major == "Biology", ]
hist(nursing$diff_hw)
hist(biology$diff_hw)
t.test(nursing$diff_hw, biology$diff_hw)
qt(0.95, 25)
qt(0.05, 25)
qt(0.05, 15)
leftright <- read.csv("C:/Users/Wanli/OneDrive/R/FoundationsOfDataAnalysisPart2/leftright.csv")
View(leftright)
sd(leftright$left)
sd(leftright$right)
leftright$diff = leftright$left - leftright$right
sd(leftright$diff)
sd(leftright$diff)/sqrt(16)
t.test(leftright$diff, alternative = 'greater')
t.test(leftright$diff)
chisq.test(c(38, 28, 24))
chisqt?
fdf
# VIDEO 4
# Read in data
wine = read.csv("wine.csv")
setwd("C:/Users/Wanli/OneDrive/R/theAnalyticsEdge/Unit02")
wine = read.csv("wine.csv")
str(wine)
summary(wine)
# Linear Regression (one variable)
model1 = lm(Price ~ AGST, data=wine)
summary(model1)
# Sum of Squared Errors
model1$residuals
SSE = sum(model1$residuals^2)
SSE
# Linear Regression (two variables)
model2 = lm(Price ~ AGST + HarvestRain, data=wine)
summary(model2)
# Sum of Squared Errors
SSE = sum(model2$residuals^2)
SSE
# Linear Regression (all variables)
model3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
# Sum of Squared Errors
SSE = sum(model3$residuals^2)
SSE
# VIDEO 5
# Remove FrancePop
model_q = lm(Price ~ HarvestRain + WinterRain, data=wine)
summary(model_q)
# Sum of Squared Errors
SSE = sum(model_q$residuals^2)
SSE
model4 = lm(Price ~ AGST + HarvestRain + WinterRain + Age, data=wine)
summary(model4)
model5 = lm(Price ~ AGST + HarvestRain + WinterRain, data=wine)
summary(model5)
model_q = lm(Price ~ HarvestRain + WinterRain, data=wine)
summary(model_q)
cor(wine$WinterRain, wine$HarvestRain)
teamRank = c(1,2,3,3,4,4,4,4,5,5)
teamWins = c(94, 88, 95, 88, 93, 94, 98, 97, 93, 94)
cor(teamRank, teamWins)
teamRank = c(1,2,3,3,4,4,4,4,5,5)
teamWins = c(97, 97, 92, 93, 92, 96, 94, 96, 92, 90)
cor(teamRank, teamWins)
# VIDEO 1
# Read in the data
NBA = read.csv("NBA_train.csv")
str(NBA)
climate = read.csv(" climate_change.csv")
str(climate)
climate = read.csv("climate_change.csv")
str(climate)
climate = read.csv("climate_change.csv")
str(climate)
climate_train = climate[climate$Year <= 2006,]
climate_test = climate[climate$Year > 2006,]
climate_model1 = lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI, data = climate_train)
summary(climate_model1)
climate_model1 = lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climate_train)
summary(climate_model1)
cor(climate_train)
cor(climate_train)
climate_model2 = lm(Temp ~ MEI + N2O + TSI + Aerosols, data = climate_train)
summary(climate_model2)
climate_model_new = step(climate_model1)
summary(climate_model_new)
summary(predict(climate_model_new, data = climate_test))
climate_predict = predict(climate_model_new, data = climate_test)
SSE = sum((climate_predict - climate_test$Temp)^2)
SST = sum((mean(climate_test$Temp) - climate_test$Temp)^2)
R2 = 1 - SSE/SST
R2
climate_predict = predict(climate_model_new, newdata = climate_test)
SSE = sum((climate_predict - climate_test$Temp)^2)
SST = sum((mean(climate_test$Temp) - climate_test$Temp)^2)
R2 = 1 - SSE/SST
R2
SST = sum((mean(climate_train$Temp) - climate_test$Temp)^2)
R2 = 1 - SSE/SST
R2
ls
ls()
dir
dir()
pisa_train = read.csv("pisa2009train.csv")
pisa_test = read.csv("pisa2009test.csv")
tapply(pisa_train$male, mean)
tapply?
?tapply
tapply()?
?tapply()
tapply(pisa_train)
tapply?
vector
tapply(pisa_train$readingScore, male, mean)
tapply(pisa_train$readingScore, male, mean())
tapply(pisa_train$readingScore, male, function = mean)
tapply(pisa_train$readingScore, pisa_train$male, function = mean)
tapply(pisa_train$readingScore, pisa_train$male, mean)
summary(pisa_train)
str(pisa_train)
pisaTrain = na.omit(pisaTrain)
pisatrain = na.omit(pisatrain)
pisa_train = na.omit(pisa_train)
pisa_test = na.omit(pisa_test)
summary(pisa_train)
summary(pisa_train$raceeth)
pisa_train$raceeth = relevel(pisa_train$raceeth, "White")
pisa_test$raceeth = relevel(pisa_test$raceeth, "White")
View(pisa_test)
View(pisa_train)
pisaLR = lm(Y ~ ., data = pisa_train)
pisaLR = lm(readingScore ~ ., data = pisa_train)
summary(pisaLR)
SSE = sum(pisaLR$residuals^2)
RMSE = sqrt(SSE/nrow(pisa_train))
RMSE
pisa_predict = predict(pisaLR, newdata = pisa_test)
summary(pisa_predict)
max(pisa_predict) - min(pisa_predict)
test_SSE = sum(pisa_predict$residuals^2)
test_SSE = sum((pisa_test$readingScore - pisa_predict)^2)
test_SSE
test_RMSE = sqrt(test_SSE/nrow(pisa_test))
test_RMSE
mean(pisa_train$readingScore)
SST = sum((pisa_test$readingScore - mean(pisa_train$readingScore))^2)
SST
R-squared = 1 - test_SSE/SST
R_squared = 1 - test_SSE/SST
R_squared
flu = read.csv("FluTrain.csv")
View(flu)
which(max(flu$ILI))
which(flu$ILI, max(flu$ILI))
which?
?which
?which()
?where()
?who()
which(flu$ILI == max(flu$ILI))
flu[which(flu$ILI == max(flu$ILI))]
flu[which(flu$ILI == max(flu$ILI)),]
flu[which(flu$Queries == max(flu$Queries)),]
hist(flu$ILI)
hist(log(flu$ILI))
plot(flu$Queries, flu$ILI)
plot(flu$Queries, log(flu$ILI))
fluLR = lm(log(flu$ILI) ~ flu$Queries)
summary(fluLR)
flutest = read.csv("FluTest.csv")
predtest1 = predict(fluLR, newdata=flutest)
predtest1
predtest1 = exp(predict(fluLR, newdata=flutest))
predtest1
predtest1[which(flutest$Week == "2012-03-11 - 2012-03-17")]
View(pisa_test)
View(pisa_test)
View(flutest)
fluLR = lm(log(flu$ILI) ~ flu$Queries, data = flu)
summary(fluLR)
flutest = read.csv("FluTest.csv")
predtest1 = exp(predict(fluLR, newdata=flutest))
predtest1
predtest1[which(flutest$Week == "2012-03-11 - 2012-03-17")]
fluLR = lm(log(flu$ILI) ~ flu$Queries, data = flu)
summary(fluLR)
flutest = read.csv("FluTest.csv")
predtest1 = exp(predict(fluLR, newdata=flutest))
predtest1[which(flutest$Week == "2012-03-11 - 2012-03-17")]
fluLR = lm(log(ILI) ~ Queries, data = flu)
summary(fluLR)
flutest = read.csv("FluTest.csv")
predtest1 = exp(predict(fluLR, newdata=flutest))
predtest1
predtest1[which(flutest$Week == "2012-03-11 - 2012-03-17")]
flutest$ILI[11]
(flutest$ILI[11] - predtest1[which(flutest$Week == "2012-03-11 - 2012-03-17")])/flutest$ILI[11]
SSE = sum((predtest1 - flutest$ILI)^2)
RMSE = sqrt(SSE/nrow(flutest))
RMSE
install.packages("zoo")
library(zoo)
ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
ILILag2 = lag(zoo(flu$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)
flu$ILILag2 = coredata(ILILag2)
summary(ILILag2)
plot(log(flu$ILI), ILILag2)
plot(log(flu$ILI), log(ILILag2)
plot(log(flu$ILI), log(ILILag2))
plot(log(flu$ILI), log(ILILag2))
model2 = lm(log(ILI) ~ Queries, data = flu)
summary(model2)
model2 = lm(log(ILI) ~ Queries + log(ILILag2), data = flu)
summary(model2)
View(flu)
?coredata
ILILag2_test = lag(zoo(flutest$ILI), -2, na.pad=TRUE)
flutest$ILILag2 = coredata(ILILag2_test)
summary(ILILag2)
summary(ILILag2_test)
