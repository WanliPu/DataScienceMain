fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
### Multiple linear regression
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
### Nonlinear terms and Interactions
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
fit5=lm(medv~l(lstat*age),Boston)
summary(fit5)
fit5=lm(medv~l(lstat*age),Boston)
fit5=lm(medv~1(lstat*age),Boston)
fit5=lm(medv~I(lstat*age),Boston)
summary(fit5)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
attach(Smarket)
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
#Fit smaller model
glm.fit=glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
require(ISLR)
names(Smarket)
summary(Smarket)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
?attach
Direction
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
#Fit smaller model
glm.fit=glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
Direction
Direction
require(ISLR)
names(Smarket)
Direction
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
#attach(Smarket)
#attach(Smarket)
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
mean(glm.pred==Direction)
load("C:/Users/Wanli/OneDrive/R/5.R.RData")
View(Xy)
View(Xy)
Xy
names(Xy)
?Xy
plot(y~.,Xy)
fit1=lm(y~.,data=Xy)
fit1
summary(fit1)
matplot(Xy, type = "l")
matplot(Xy, type = "l", legend = True)
matplot(Xy, type = "l", legend = T)
?matplot
View(Xy)
require(ISLR)
require(boot)
?cv.glm
plot(mpg~horsepower,data=Auto)
## LOOCV
?glm
glm.fit=glm(mpg~horsepower, data=Auto)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)
require(ISLR)
require(boot)
?cv.glm
plot(mpg~horsepower,data=Auto)
?glm
## LOOCV
glm.fit=glm(mpg~horsepower, data=Auto)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)
?cv.glm
?loocv
loocv=function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
## Now we try it out
loocv(glm.fit)
cv.error=rep(0,5)
degree=1:5
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d]=loocv(glm.fit)
}
plot(degree,cv.error,type="b")
## 10-fold CV
cv.error10=rep(0,5)
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error10[d]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
lines(degree,cv.error10,type="b",col="red")
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)
## What is the standard error of alpha?
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
Xy
names(Xy)
?Xy
fit1=lm(y~.,data=Xy)
fit1
summary(fit1)
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
matplot(Xy, type = "l", legend = T)
Xy
names(Xy)
?Xy
fit1=lm(y~.,data=Xy)
fit1
summary(fit1)
matplot(Xy, type = "l", legend = T)
?matplot
matplot(Xy, type = "l", legend = T)
alpha(X1,y)
alpha(Xy$X1,Xy$y)
## What is the standard error of alpha?
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Xy,1:100)
?with
alpha.fn=function(data, index){
with(data[index,],alpha(X1,Y))
}
with(data[index,],alpha(x1,y))
alpha.fn=function(data, index){
with(data[index,],alpha(x1,y))
}
alpha.fn(Xy,1:100)
alpha.fn=function(data, index){
with(data[index,],alpha(Xy$x1,y))
}
alpha.fn(Xy,1:100)
alpha.fn=function(data, index){
with(data[index,],alpha(Xy$X1,Xy$y))
}
alpha.fn(Xy,1:100)
set.seed(1)
alpha.fn (Xy,sample(1:100,100,replace=TRUE))
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
plot(boot.out)
# Get the current directory
getwd()
ls()
# Read the csv file
mvt = read.csv("mvtWeek1.csv")
# Structure of the dataset
str(mvt)
# Statistical summary
summary(mvt)
table(mvt$Arrest)
Alley =  mvt[mvt$LocationDescription == "ALLEY", ]
str(Alley)
# Now, let's convert these characters into a Date object in R. In your R console, type
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
table(mvt$Month)
min(table(mvt$Month))
table(mvt$Weekday)
max(table(mvt$Weekday))
table(mvt$Month, mvt$Arrest)
hist(mvt$Date, breaks=100)
getwd()
setwd("C:/Users/Wanli/OneDrive/R/theAnalyticsEdge")
getwd()
ls()
# Read the csv file
mvt = read.csv("mvtWeek1.csv")
# Structure of the dataset
str(mvt)
# Statistical summary
summary(mvt)
table(mvt$Arrest)
Alley =  mvt[mvt$LocationDescription == "ALLEY", ]
str(Alley)
# Now, let's convert these characters into a Date object in R. In your R console, type
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
table(mvt$Month)
min(table(mvt$Month))
table(mvt$Weekday)
max(table(mvt$Weekday))
table(mvt$Month, mvt$Arrest)
hist(mvt$Date, breaks=100)
boxplot(mvt$Date ~ mvt$Arrest)
boxplot(mvt$Arrest ~ mvt$Date)
table(mvt$Year)
table?
table()?
?table()
?table
table(mvt$Year, mvt$Arrest)
prop.table(mvt$Year, mvt$Arrest)
prop.table(year_arrest)
year_arrest = table(mvt$Year, mvt$Arrest)
prop.table(year_arrest)
table(mvt$Year, mvt$Arrest, useNA = "ifany")
prop.table(year_arrest2)
year_arrest2 = table(mvt$Year, mvt$Arrest, useNA = "ifany")
prop.table(year_arrest2)
prop.table(year_arrest, axis = 1)
?prop.table
prop.table(year_arrest, 1)
prop.table(year_arrest, 0)
prop.table(year_arrest, 1)
prop.table(year_arrest, 2)
prop.table(year_arrest, 1)
sort(table(mvt$LocationDescription))
Top5 = mvt[mvt$LocationDescription == "Gas Station" | mvt$LocationDescription == "Street" | mvt$LocationDescription == "Parking Lot/Garage (Non-Residential)" | mvt$LocationDescription == "Alley" | mvt$LocationDescription == "Driveway (Residential)", ]
View(Top5)
View(Top5)
Top5 = mvt[mvt$LocationDescription == "Gas Station", ]
Top5 = mvt[mvt$LocationDescription == "Gas Station", ]
Top5 = mvt[mvt$LocationDescription == "Street", ]
Top5 = mvt[mvt$LocationDescription == "STREET", ]
mvt$LocationDescription == "Parking Lot/Garage (Non-Residential)" |
mvt$LocationDescription == "Alley" |
mvt$LocationDescription == "Driveway (Residential)", ]
Top5 = mvt[mvt$LocationDescription == "GAS STATION" |
mvt$LocationDescription == "STREET" |
mvt$LocationDescription == "PARKING LOT/GARAGE(NON.RESID.)" |
mvt$LocationDescription == "ALLEY" |
mvt$LocationDescription == "DRIVEWAY - RESIDENTIAL", ]
str(Top5)
table(Top5$LocationDescription)
View(Top5)
table(Top5$LocationDescription)
table(Top5$LocationDescription)
str(Top5)
Top5$LocationDescription = factor(Top5$LocationDescription)
table(Top5$LocationDescription)
table(Top5$LocationDescription, Top5$Arrest)
top5table = table(Top5$LocationDescription, Top5$Arrest)
prop.table(top5table)
prop.table(top5table, 1)
table(Top5$Date)
table(Top5$Weekday)
table(Top5[Top5$LocationDescription == "GAS STATION",]$Weekday)
table(Top5[Top5$LocationDescription == "DRIVEWAY - RESIDENTIAL",]$Weekday)
dir()
clc
clc()
clear()
setwd("C:/Users/Wanli/OneDrive/dsmain/theAnalyticsEdge")
ds = read.csv('data_scientist')
ds = read.csv('data_scientist.csv')
str(ds)
summary(ds)
ds[which.min(ds)]
ds[which.min(ds$salary)]
ds[which.min(ds$salary), ]
hist(ds$salary)
ds[which.max(ds$salary), ]
certified = subset(ds, ds$status=='CERTIFIED')
hist(ds$salary)
hist(certified$salary)
?hist
edges <- read.csv("edges.csv")
setwd("C:/Users/Wanli/OneDrive/dsmain/theAnalyticsEdge/Unit07_Visualization")
edges <- read.csv("edges.csv")
users <- read.csv("users.csv")
edges <- read.csv("edges.csv")
users <- read.csv("users.csv")
View(edges)
View(users)
146*2/59
str(users)
# In our dataset, what is the average number of friends per user?
# We have 146*2 connections (friends) and 59 nodes (users) and this is the clue
# Out of all the students who listed a school, what was the most common locale?
table(users$locale)
# Is it possible that either school A or B is an all-girls or all-boys school?
table(users$school,users$gender)
##################################
# Problem 2 - Creating a Network
install.packages("igraph")
library(igraph)
?graph.data.frame
g <- graph.data.frame(edges, directed=F, users)
plot(g, vertex.size=5, vertex.label=NA)
degree(g)
V(g)$size = degree(g)/2+2
plot(g, vertex.label=NA)
# What is the largest size we assigned to any node in our graph?
max(V(g)$size)
# What is the smallest size we assigned to any node in our graph?
min(V(g)$size)
V(g)$color = "black"
V(g)$color[V(g)$gender == "A"] = "red"
V(g)$color[V(g)$gender == "B"] = "gray"
# Plot the resulting graph. What is the gender of the users with the highest degree in the graph?
plot(g, vertex.label=NA)
plot(g, vertex.label=NA)
V(g)$color[V(g)$school == ""] = "white"
V(g)$color[V(g)$school == "A"] = "red"
V(g)$color[V(g)$school == "AB"] = "green"
plot(g, vertex.label=NA)
V(g)$color[V(g)$locale == ""] = "white"
V(g)$color[V(g)$locale == "A"] = "red"
V(g)$color[V(g)$locale == "B"] = "green"
# The large connected component is most associated with which locale?
plot(g, vertex.label=NA)
?igraph.plotting
tweets <- read.csv("tweets.csv", stringsAsFactors=F)
# 1) Create a corpus using the Tweet variable
library(tm)
install.packages(tm)
library(tm)
install.packages("tm")
library(tm)
corpus <- Corpus(VectorSource(tweets$Tweet))
# 2) Convert the corpus to lowercase
corpus <- tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
# 3) Remove punctuation from the corpus
corpus <- tm_map(corpus, removePunctuation)
# 4) Remove all English-language stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# 5) Build a document-term matrix out of the corpus
dtm <- DocumentTermMatrix(corpus)
# 6) Convert the document-term matrix to a data frame called allTweets
allTweets <- as.data.frame(as.matrix(dtm))
colnames(allTweets) <- make.names(colnames(allTweets))
# How many unique words are there across all the documents?
ncol(allTweets)
install.packages("wordcloud")
library(wordcloud)
# Which function can we apply to allTweets to get a vector of the words in our dataset,
# which we'll pass as the first argument to wordcloud()?
?wordcloud
wordcloud(colnames(allTweets),colSums(allTweets),scale=c(2,0.5),min.freq=20,colors=brewer.pal(8, "Dark2"))
# Pre-process the corpus, this time removing the most frequent word
corpus <- Corpus(VectorSource(tweets$Tweet))
# 2) Convert the corpus to lowercase
corpus <- tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
# 3) Remove punctuation from the corpus
corpus <- tm_map(corpus, removePunctuation)
# 4) Remove all English-language stopwords and the most frequent word
corpus <- tm_map(corpus, removeWords, c("apple",stopwords("english")))
# 5) Build a document-term matrix out of the corpus
dtm <- DocumentTermMatrix(corpus)
# 6) Convert the document-term matrix to a data frame called allTweets
allTweets <- as.data.frame(as.matrix(dtm))
colnames(allTweets) <- make.names(colnames(allTweets))
wordcloud(colnames(allTweets),colSums(allTweets),scale=c(2,0.25),colors=brewer.pal(8, "Dark2"))
wordcloud(colnames(allTweets),colSums(allTweets),scale=c(2,0.5),min.freq=20,colors=brewer.pal(8, "Dark2"))
corpus <- Corpus(VectorSource(tweets$Tweet))
?wordcloud
wordcloud(colnames(allTweets),colSums(allTweets),scale=c(2,0.5),min.freq=20,colors=brewer.pal(8, "Dark2"))
corpus <- Corpus(VectorSource(tweets$Tweet))
str(corpus)
str(allTweets )
corpus[[1]]
allTweets$Avg <- tweets$Avg
# Avg that is negative implies that the data contains negative words
wordcloud(colnames(allTweets[allTweets$Avg<0,1:3779]),colSums(allTweets),scale=c(2,0.25),colors=brewer.pal(9, "YlOrRd"))
dtm <- DocumentTermMatrix(corpus)
corpus
corpus[[1]]
allTweets$Avg <- tweets$Avg
wordcloud(colnames(allTweets[allTweets$Avg<0,1:3779]),colSums(allTweets),scale=c(2,0.25),colors=brewer.pal(9, "YlOrRd"))
wordcloud(colnames(allTweets),colSums(allTweets),scale=c(2,0.25),colors=brewer.pal(9, "YlOrRd")[c(-1,-2,-3,-4)],random.order=F)
wordcloud(colnames(allTweets),colSums(allTweets),scale=c(2,0.25),colors=brewer.pal(9, "YlOrRd"),rot.per=0.4)
?wordcloud
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(RColorBrewer)
# Which color palette would be most appropriate for use in a word cloud for which we want to use color
# to indicate word frequency?
display.brewer.all()
brewer.pal(9, "Blues")[c(-5, -6, -7, -8, -9)]
tweets <- read.csv("tweets.csv", stringsAsFactors=F)
# 1) Create a corpus using the Tweet variable
library(tm)
corpus <- Corpus(VectorSource(tweets$Tweet))
# 2) Convert the corpus to lowercase
corpus <- tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
# 3) Remove punctuation from the corpus
corpus <- tm_map(corpus, removePunctuation)
# 4) Remove all English-language stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# 5) Build a document-term matrix out of the corpus
dtm <- DocumentTermMatrix(corpus)
# 6) Convert the document-term matrix to a data frame called allTweets
allTweets <- as.data.frame(as.matrix(dtm))
View(allTweets)
colnames(allTweets) <- make.names(colnames(allTweets))
# How many unique words are there across all the documents?
ncol(allTweets)
# What is the most compelling rationale for skipping the process of stemming words
# when visualizing text data?
# Use logic to answer the question
###########################################
# Problem 2- Building a Word Cloud
# Install and load the "wordcloud" package, which is needed to build word clouds.
library(wordcloud)
# Which function can we apply to allTweets to get a vector of the words in our dataset,
# which we'll pass as the first argument to wordcloud()?
?wordcloud
# Which function should we apply to allTweets to obtain the frequency of each word across all tweets?
# function that return the sum of each column
# Use allTweets to build a word cloud.
wordcloud(colnames(allTweets),colSums(allTweets),scale=c(2,0.5),min.freq=20,colors=brewer.pal(8, "Dark2"))
