linFitPred(time, mv, 12)
View(fa100k2)
View(world)
us <- world[world$Country.Code == "USA",]
# Select the years from 1990 and name the new data frame "us_select"
us_select <- us[us$year >= 1990, ]
# Create a new variable in our datset called internet.mil to make the number of users more interpretable (into millions)
us_select$internet.mil <- us_select$internet.users / 1000000
# Create a new variable in our dataset called time that represents "years since 1990"
us_select$time <- us_select$year - 1990
# Select the first 10 years (from 1990 to 1999) and name the new data frame "us_select_10"
us_select_10 <- us_select[us_select$time < 10,]
expFit(us_select_10$time, us_select_10$internet.mil)
logisticFit(us_select_10$time, us_select_10$internet.mil)
View(us_select_10)
View(us_select)
e <- expFitPred(us_select_10$time, us_select_10$internet.mil, 16)
l <- logisticFitPred(us_select_10$time, us_select_10$internet.mil, 16)
den <- world[world$Country.Code == 'Denmark',]
View(world)
den <- world[world$Country.Code == "Denmark",]
den <- world[world$Country == "Denmark",]
den_select <- den[den$year >= 1990, ]
den_select$yearsince1990 = den_select$year - 1990
expFit(den_select$yearsince1990, den_select$internet.users)
den_select$internet.prop = den_select$internet.users / den_select$population
View(den_select)
expFit(den_select$yearsince1990, den_select$internet.prop)
logisticFit(den_select$yearsince1990, den_select$internet.prop)
e <- expFitPred(den_select$yearsince1990, den_select$internet.prop, 0.7)
e <- expFitPred(den_select$yearsince1990, den_select$internet.prop, 15)
e <- expFitPred(den_select$yearsince1990, den_select$internet.prop, 19)
e <- expFitPred(den_select$yearsince1990, den_select$internet.prop, 16)
e <- expFitPred(den_select$yearsince1990, den_select$internet.prop, 17)
e <- logisticFitPred(den_select$yearsince1990, den_select$internet.prop, 17)
e <- logisticFitPred(den_select$yearsince1990, den_select$internet.prop, 12)
e <- logisticFitPred(den_select$yearsince1990, den_select$internet.prop, 13)
e <- expFitPred(den_select$yearsince1990, den_select$internet.prop, 0)
bra <- world[world$Country.Code == "BRA",]
bra_select = bra[bra$year >= 1995, ]
bra_select$time = bra_select$year - 1995
bra_select$user_mil = bra_select$mobile.users / 1000000
View(bra_select)
expFit(bra_select$time, bra_select$user_mil)
linFit(bra_select$time, bra_select$user_mil)
LogisticFit(bra_select$time, bra_select$user_mil)
logisticFit(bra_select$time, bra_select$user_mil)
logisticFitPred(bra_select$time, bra_select$user_mil, 30)
time = c(1, 3)
wolves = c(25, 45)
expFit(time, wolves)
expFit(time, wolves, 0)
expFitPred(time, wolves, 0)
expFitPred(time, wolves, 7)
expFitPred(time, wolves, 10)
expFitPred(time, wolves, 9)
x = c(2, 7, 5)
x
y = seq(from = 4, length = 3, by = 3)
y
?seq
x + y
x/y
x^y
x[2]
x[-2]
x
x[-c(1, 2)]
x
z = matrix(seq(1, 12), 4, 3)
z
ls()
clc
clc()
clear()
rm()
ls()
x = runif(50)
y= rnorm(50)
plot(x, y)
?runif
library(MASS)
library(ISLR)
install.packages("ISLR")
library(ISLR)
names(Boston)
?Boston
plot(medv~lstat,Boston)
plot(Boston$medv, Boston$lstat)
plot(Boston$lstat, Boston$medv)
fit1=lm(medv~lstat,data=Boston)
fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
### Multiple linear regression
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
### Nonlinear terms and Interactions
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
fit5=lm(medv~l(lstat*age),Boston)
summary(fit5)
fit5=lm(medv~l(lstat*age),Boston)
fit5=lm(medv~1(lstat*age),Boston)
fit5=lm(medv~I(lstat*age),Boston)
summary(fit5)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
attach(Smarket)
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
#Fit smaller model
glm.fit=glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
require(ISLR)
names(Smarket)
summary(Smarket)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
?attach
Direction
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
#Fit smaller model
glm.fit=glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
Direction
Direction
require(ISLR)
names(Smarket)
Direction
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
#attach(Smarket)
#attach(Smarket)
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
mean(glm.pred==Direction)
load("C:/Users/Wanli/OneDrive/R/5.R.RData")
View(Xy)
View(Xy)
Xy
names(Xy)
?Xy
plot(y~.,Xy)
fit1=lm(y~.,data=Xy)
fit1
summary(fit1)
matplot(Xy, type = "l")
matplot(Xy, type = "l", legend = True)
matplot(Xy, type = "l", legend = T)
?matplot
View(Xy)
require(ISLR)
require(boot)
?cv.glm
plot(mpg~horsepower,data=Auto)
## LOOCV
?glm
glm.fit=glm(mpg~horsepower, data=Auto)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)
require(ISLR)
require(boot)
?cv.glm
plot(mpg~horsepower,data=Auto)
?glm
## LOOCV
glm.fit=glm(mpg~horsepower, data=Auto)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)
?cv.glm
?loocv
loocv=function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
## Now we try it out
loocv(glm.fit)
cv.error=rep(0,5)
degree=1:5
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d]=loocv(glm.fit)
}
plot(degree,cv.error,type="b")
## 10-fold CV
cv.error10=rep(0,5)
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error10[d]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
lines(degree,cv.error10,type="b",col="red")
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)
## What is the standard error of alpha?
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
Xy
names(Xy)
?Xy
fit1=lm(y~.,data=Xy)
fit1
summary(fit1)
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
matplot(Xy, type = "l", legend = T)
Xy
names(Xy)
?Xy
fit1=lm(y~.,data=Xy)
fit1
summary(fit1)
matplot(Xy, type = "l", legend = T)
?matplot
matplot(Xy, type = "l", legend = T)
alpha(X1,y)
alpha(Xy$X1,Xy$y)
## What is the standard error of alpha?
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Xy,1:100)
?with
alpha.fn=function(data, index){
with(data[index,],alpha(X1,Y))
}
with(data[index,],alpha(x1,y))
alpha.fn=function(data, index){
with(data[index,],alpha(x1,y))
}
alpha.fn(Xy,1:100)
alpha.fn=function(data, index){
with(data[index,],alpha(Xy$x1,y))
}
alpha.fn(Xy,1:100)
alpha.fn=function(data, index){
with(data[index,],alpha(Xy$X1,Xy$y))
}
alpha.fn(Xy,1:100)
set.seed(1)
alpha.fn (Xy,sample(1:100,100,replace=TRUE))
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
plot(boot.out)
# Get the current directory
getwd()
ls()
# Read the csv file
mvt = read.csv("mvtWeek1.csv")
# Structure of the dataset
str(mvt)
# Statistical summary
summary(mvt)
table(mvt$Arrest)
Alley =  mvt[mvt$LocationDescription == "ALLEY", ]
str(Alley)
# Now, let's convert these characters into a Date object in R. In your R console, type
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
table(mvt$Month)
min(table(mvt$Month))
table(mvt$Weekday)
max(table(mvt$Weekday))
table(mvt$Month, mvt$Arrest)
hist(mvt$Date, breaks=100)
getwd()
setwd("C:/Users/Wanli/OneDrive/R/theAnalyticsEdge")
getwd()
ls()
# Read the csv file
mvt = read.csv("mvtWeek1.csv")
# Structure of the dataset
str(mvt)
# Statistical summary
summary(mvt)
table(mvt$Arrest)
Alley =  mvt[mvt$LocationDescription == "ALLEY", ]
str(Alley)
# Now, let's convert these characters into a Date object in R. In your R console, type
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
table(mvt$Month)
min(table(mvt$Month))
table(mvt$Weekday)
max(table(mvt$Weekday))
table(mvt$Month, mvt$Arrest)
hist(mvt$Date, breaks=100)
boxplot(mvt$Date ~ mvt$Arrest)
boxplot(mvt$Arrest ~ mvt$Date)
table(mvt$Year)
table?
table()?
?table()
?table
table(mvt$Year, mvt$Arrest)
prop.table(mvt$Year, mvt$Arrest)
prop.table(year_arrest)
year_arrest = table(mvt$Year, mvt$Arrest)
prop.table(year_arrest)
table(mvt$Year, mvt$Arrest, useNA = "ifany")
prop.table(year_arrest2)
year_arrest2 = table(mvt$Year, mvt$Arrest, useNA = "ifany")
prop.table(year_arrest2)
prop.table(year_arrest, axis = 1)
?prop.table
prop.table(year_arrest, 1)
prop.table(year_arrest, 0)
prop.table(year_arrest, 1)
prop.table(year_arrest, 2)
prop.table(year_arrest, 1)
sort(table(mvt$LocationDescription))
Top5 = mvt[mvt$LocationDescription == "Gas Station" | mvt$LocationDescription == "Street" | mvt$LocationDescription == "Parking Lot/Garage (Non-Residential)" | mvt$LocationDescription == "Alley" | mvt$LocationDescription == "Driveway (Residential)", ]
View(Top5)
View(Top5)
Top5 = mvt[mvt$LocationDescription == "Gas Station", ]
Top5 = mvt[mvt$LocationDescription == "Gas Station", ]
Top5 = mvt[mvt$LocationDescription == "Street", ]
Top5 = mvt[mvt$LocationDescription == "STREET", ]
mvt$LocationDescription == "Parking Lot/Garage (Non-Residential)" |
mvt$LocationDescription == "Alley" |
mvt$LocationDescription == "Driveway (Residential)", ]
Top5 = mvt[mvt$LocationDescription == "GAS STATION" |
mvt$LocationDescription == "STREET" |
mvt$LocationDescription == "PARKING LOT/GARAGE(NON.RESID.)" |
mvt$LocationDescription == "ALLEY" |
mvt$LocationDescription == "DRIVEWAY - RESIDENTIAL", ]
str(Top5)
table(Top5$LocationDescription)
View(Top5)
table(Top5$LocationDescription)
table(Top5$LocationDescription)
str(Top5)
Top5$LocationDescription = factor(Top5$LocationDescription)
table(Top5$LocationDescription)
table(Top5$LocationDescription, Top5$Arrest)
top5table = table(Top5$LocationDescription, Top5$Arrest)
prop.table(top5table)
prop.table(top5table, 1)
table(Top5$Date)
table(Top5$Weekday)
table(Top5[Top5$LocationDescription == "GAS STATION",]$Weekday)
table(Top5[Top5$LocationDescription == "DRIVEWAY - RESIDENTIAL",]$Weekday)
setwd("C:/Users/Wanli/OneDrive/dsmain/theAnalyticsEdge/Unit05_Text_Analytics")
tweets = read.csv("tweets.csv", stringsAsFactors=FALSE)
View(tweets)
table(tweets$Negative)
tweets$Negative = as.factor(tweets$Avg <= -1)
table(tweets$Negative)
install.packages("tm")
library(tm)
install.packages("SnowballC")
library(SnowballC)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus
corpus[[1]]
corpus = tm_map(corpus, tolower)
corpus[[1]]
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus[[1]]
corpus[[1]]
stopwords("english")[1:10]
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus[[1]]
corpus = tm_map(corpus, stemDocument)
corpus[[1]]
frequencies = DocumentTermMatrix(corpus)
frequencies
inspect(frequencies[1000:1005,505:515])
findFreqTerms(frequencies, lowfreq=20)
sparse = removeSparseTerms(frequencies, 0.995)
sparse
tweetsSparse = as.data.frame(as.matrix(sparse))
View(tweetsSparse)
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
View(tweetsSparse)
tweetsSparse$Negative = tweets$Negative
library(caTools)
set.seed(123)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
trainSparse = subset(tweetsSparse, split==TRUE)
testSparse = subset(tweetsSparse, split==FALSE)
findFreqTerms(frequencies, lowfreq=100)
library(rpart)
library(rpart.plot)
tweetCART = rpart(Negative ~ ., data=trainSparse, method="class")
prp(tweetCART)
predictRF = predict(tweetRF, newdata=testSparse)
table(testSparse$Negative, predictCART)
predictCART = predict(tweetCART, newdata=testSparse, type="class")
table(testSparse$Negative, predictCART)
# Compute accuracy
(294+18)/(294+6+37+18)
# Baseline accuracy
table(testSparse$Negative)
300/(300+55)
library(randomForest)
set.seed(123)
tweetRF = randomForest(Negative ~ ., data=trainSparse)
# Make predictions:
predictRF = predict(tweetRF, newdata=testSparse)
table(testSparse$Negative, predictRF)
(293+21)/(293+7+34+21)
predictions = predict(tweetLog, newdata=testSparse, type="response")
tweetlog = glm(Negative ~., ddata = trainSparse)
tweetlog = glm(Negative ~., data = trainSparse)
predictions = predict(tweetLog, newdata=testSparse, type="response")
tweetlog = glm(Negative ~., data = trainSparse, family = "binomial")
predictions = predict(tweetLog, newdata=testSparse, type="response")
tweetLog = glm(Negative ~., data = trainSparse, family = "binomial")
predictions = predict(tweetLog, newdata=testSparse, type="response")
table(testSparse$Negative, predictions>=0.5)
(253+33)/nrow(testSparse)
ls
dir
ls()
dir()
